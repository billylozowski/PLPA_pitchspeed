---
title: "Predicting Pitched Ball Velocity in Baseball"
author: "Billy Lozowski"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
  md_document:
      variant: gfm
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(1618) # ensure all randomly generated data is repeatable
# as a side joke, this seed is the year in which the 'second defenestration of Prague' occurred
```

[_How to install Java_](https://www.java.com/en/download/)

## Load Packages

Before we do anything, we need load the packages required for our analysis.

```{r include=FALSE} 
# + If the packages aren't already installed, run the following code:
# 
# install.packages("tidyverse")
# 
# install.packages("tictoc")
# 
# Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jdk-<version>")
# install.packages("rJava")
# 
# install.packages("glmulti")
# 
# install.packages("party")
# 
# devtools::install_github("dustinfife/flexplot")*
# 
# devtools::install_github("strengejacke/strengejacke")
# 
# install.packages("sjPlot")
```

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(rJava)
library(glmulti)
library(party)
library(flexplot)
library(tictoc)
library(flextable)
library(sjPlot)
library(performance)
```

## Load Data

We'll load our data "Pitch Speed.csv" and create the object "ball.speed".

```{r data}
ball.speed <- read.csv("Pitch Speed.csv", header = TRUE)
```

## Filtering Fastball-Only Data

We're going to subset the data to include only 'fastball' pitches, and the variables we're interested in in relation to this pitch type. 

```{r filter data}
# subset data
ball.speed.subset <- ball.speed %>%
  filter(Pitch.Type == "Fastball") %>%
  # select only columns with data we're interested in
  select(RelSpeed..m.s., 24:ncol(ball.speed))
```

## Visualise Data (1)

Before we clean the data, we'll visualise it to get an idea of where any potential outliers might be.

```{r data cleaning, echo=FALSE}
# pivot the data frame
ball.speed.subset.long <- ball.speed.subset %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") 

# plot the data
ggplot(ball.speed.subset.long, aes(variable, values)) +
  geom_boxplot() +
  theme_classic() +
  theme(axis.text.x = element_blank())
```

## Visualise Data (2)

Our first plot doesn't really help us with values on the lower end because of some extreme values. This is mainly due to one particular variable "max trunk rotation acceleration", so we'll remove this variable and re-plot the data.

```{r replotting the data, echo=FALSE}
options(scipen = 999) # prevent scientific number output

# calculate column means to see which variable is the extreme one
column.means <- as.data.frame(colMeans(ball.speed.subset))  

# remove "max trunk rotation acceleration"
ball.speed.subset <- ball.speed.subset %>%
  select(-Max.Trunk.Rotation.Acceleration..deg.s.2.)

# pivot the data frame
ball.speed.subset.long <- ball.speed.subset %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") 

# plot the data
ggplot(ball.speed.subset.long, aes(variable, values)) +
  geom_boxplot() +
  theme_classic() +
  theme(axis.text.x = element_blank())
```

This is a bit better, but it's still not very clear. Given the scale of difference in variable values, it doesn't seem appropriate to keep removing the highest values. So, instead, since outliers do seem to be present in the data, we'll go ahead and remove these.

```{r outlier removal, warning=FALSE, message=FALSE, echo=FALSE}
# loop over each numeric column in a data frame
remove_outliers <- function(data) {
  data[] <- lapply(data, function(column) {
    if (is.numeric(column)) {
      
      # calculate Q1, Q3, and IQR
      Q1 <- quantile(column, 0.25, na.rm = TRUE)
      Q3 <- quantile(column, 0.75, na.rm = TRUE)
      IQR_val <- Q3 - Q1
      
      # set thresholds for outliers
      lower_limit <- Q1 - 2 * IQR_val
      upper_limit <- Q3 + 2 * IQR_val
      
      # replace outliers with NA (or you can choose to filter them out)
      column[column < lower_limit | column > upper_limit] <- NA
    }
    return(column)
  })
  return(data)
}
```

## Remove Outliers

```{r}
# apply the function to your data frame
ball.speed.clean <- remove_outliers(ball.speed.subset)
```

## Remove "NA" values

Since we've removed outliers, we'll need to account for this so our random forest model can run properly. We'll ensure no "NA" values are included in our model.

```{r filter outliers}
# filter the missing values from the data set
ball.speed.filtered <- ball.speed.clean %>%
  filter(!if_any(everything(), is.na))
```

## Random Forest Model

To determine which variables we want to keep in our model later on, we can now construct the random forest (rf) model for pitch speed. We'll use "variable importance" [(Mahieu et al., 2023)](https://www.sciencedirect.com/science/article/abs/pii/S0169743923002368) to make our decisions!

```{r random forest model}
# construct the rf model
rf.ball.speed <- cforest(RelSpeed..m.s. ~ ., data = ball.speed.filtered)

# extract the estimates for rf.ball.speed
estimates.ball.speed <- estimates(rf.ball.speed)
```

```{r extract random forest estimates, echo=FALSE}
# create a data frame with variables and their importance in
variable.importance <- as.data.frame(estimates.ball.speed[["importance"]])

# add a new column for the variable name and re-order the columns
variable.importance <- variable.importance %>%
  rownames_to_column(var = "Variable") %>%
  select(Variable, everything())

# rename the importance column
colnames(variable.importance)[2] <- "Importance (V.I.)"

# ensure variables are arranged by their importance (highest to lowest)
variable.importance <- variable.importance %>%
  arrange(desc(`Importance (V.I.)`))%>%
  mutate(Variable = factor(Variable, levels = Variable))

head(variable.importance, 10)
```

## Plot Variable Importance (1)

Since the variables are arranged in order of their importance, we'll visualise this to see if there are any clear cut-offs (i.e. variables we should vs. variables we maybe shouldn't include).

```{r plot estimates, echo=FALSE}
ggplot(variable.importance, aes(Variable, `Importance (V.I.)`, fill = `Importance (V.I.)`)) +
geom_bar(stat = "identity") +
  theme_bw() +
  labs(x = NULL,
       y = "Variable Importance") +
  theme(axis.text.x = element_blank(),
        x.ticks = NULL)
```

From this first plot, it seems as if there might be a cut-off in terms of our variable importance. Let's zoom in and see if we can get a better idea how many variables this includes.

## Plot Variable Importance (2)

```{r subset 1, echo=FALSE}
# subset the data
subset1 <- subset(variable.importance, `Importance (V.I.)` >= 0.15)

# plot the subset data
ggplot(subset1, aes(Variable, `Importance (V.I.)`, fill = `Importance (V.I.)`)) +
geom_bar(stat = "identity") +
  theme_classic() +
  labs(x = NULL,
       y = "Variable Importance") +
  theme(axis.text.x = element_blank(),
        x.ticks = NULL)
```

```{r subset 2, echo=FALSE}
# subset the data
subset2 <- subset1 %>%
  filter(`Importance (V.I.)` >= 0.2)

# return the variable names
variable.names <- subset2$Variable 
```

## Automated Model Selection and Model-Averaging - Summary

When deciding which variables to include in the next portion of our analysis, we're going to set a V.I. threshold of >= 0.2. Though this cut-off is arbitrary (it's not based on any literature), 0.2 looks like it might be suitable. 

Six variables (below) have a V.I. of more than 0.2. As such, we'll subset the data again to include just these (i.e. V.I. values >= 0.2), and move onto our automated model selection and model-averaging analysis using the __'glmulti'__ package.

  1. Lead.Ankle.Flexion.at.FC..deg.    
  2. Stride.Orientation
  3. Step.Width..m.
  4. Center.of.Mass.A.P.at.FC.Zeroed..m.
  5. Max.Lead.Hip.Flexion.Velocity..deg.s.
  6. Lead.Hip.Rotation.at.FC..deg.

We'll output a general summary of how many regression models we'll assess.

+ _Note. RF models sample the data at random, so the V.I. values can change for each variable after each run. To ensure only those with a V.I >= 0.2 are included in our regression models below, we'll dynamically pull the variables with a V.I. value >= 0.2 from the "variable.names" object_

```{r summary of models, echo=FALSE}
glmulti(as.formula(paste("RelSpeed..m.s. ~", 
                         # dynamically input variable names from subset 2
                         paste(variable.names, collapse = " + "))), 
        data = ball.speed.filtered,
        crit = bic,          # model fit criterion
        level = 1,           # 1 without interactions, 2 with interactions
        method = "d",        # simple summary of candidates
                             # other options include "g" or "h"
        family = gaussian,
        fitfunction = glm,   # specify the model type (lm or glm)
        confsetsize = 100)   # keep the 100 best models (confidence set)
```

## Automated Model Selection and Model-Averaging - Genetic Algorithm

We'll now build our models using an genetic algorithm. This will compute a number of regression models and return the best 100. This should then give us an idea as to which combination of predictors might best explain our pitched ball velocity. Whilst we're using the genetic algorithm here, it's advised to refer to the documentation to ensure the correct/desired model parameters are set.

[__glmulti documentation__](https://www.rdocumentation.org/packages/glmulti/versions/1.0.8/topics/glmulti)

```{r model with genetic algorithm, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
tic()
h.model <- glmulti(as.formula(paste("RelSpeed..m.s. ~", 
                         # dynamically input variable names from subset 2
                         paste(variable.names, collapse = " + "))),
                   data = ball.speed.filtered,
                   crit = bic,         # model fit criterion
                   level = 2,          # 1 without interactions, 2 with interactions
                   method = "g",       # genetic screening algorithm (better with fewer predictors)
                   family = gaussian,
                   fitfunction = glm,  # specify the model type (lm or glm)
                   confsetsize = 100)  # keep the 100 best models (confidence set)
toc()
```

## Visualise the Best 100 Models

```{r model information, echo=FALSE}
# model results
print(h.model)

# plot the best 100 models base on information criterion (IC)
plot(h.model) # red line indicates 2 IC units from the best model
```

## Extracting Model Information

Let's extract some model information and see which variables the best models contain (in this case, the best 3 models).

#### Model-Averaged Importance Terms

```{r variable importance across models, echo=FALSE}
# adjust the plot margins
par(mar = c(5, 25, 1, 2) + 0.1)

# plot the model so it shows in the plot viewer
plot(h.model, type = "s")

################################################################################

# open a PNG device with specified width and height
png(filename = "Model Importance Terms.png", width = 2000, height = 1200, res = 150)

# set up the layout to include extra space for the y-axis labels
layout(matrix(1), widths = c(1.5), heights = c(1))

# adjust the plot margins
par(mar = c(5, 25, 1, 2) + 0.1)

# plot the model
plot(h.model, type = "s")

# close the PNG device
dev.off()
```

#### Best 3 Models

```{r model variables, weights, and ICs, echo=FALSE}
weightable(h.model)[1:3,] %>% # select the best models
  regulartable() %>%
  autofit()
```

__Lead.Ankle.Flexion.at.FC..deg.__, __Max.Lead.Hip.Flexion.Velocity..deg.s.:Lead.Ankle.Flexion.at.FC..deg.__, and __Max.Lead.Hip.Flexion.Velocity..deg.s.:Step.Width..m.__ appear to be the most common terms across all computed models (included in > 80%). Incidentally, they are the sole 3 terms in our highest weighted model, so we'll summarise this to finish.

## Final Model

```{r first versus second strongest models, echo=FALSE, include=FALSE}
model.one <- lm(RelSpeed..m.s. ~
                  # predictors
                  Lead.Ankle.Flexion.at.FC..deg. + 
                  # interactions
                  Max.Lead.Hip.Flexion.Velocity..deg.s.:Lead.Ankle.Flexion.at.FC..deg. +
                  Max.Lead.Hip.Flexion.Velocity..deg.s.:Step.Width..m.,
                data = ball.speed.filtered)

# model.two <- lm(RelSpeed..m.s. ~
#                   # predictors
#                   Lead.Ankle.Flexion.at.FC..deg. + 
#                   # interactions
#                   Center.of.Mass.A.P.at.FC.Zeroed..m.:Lead.Ankle.Flexion.at.FC..deg.,
#                 data = ball.speed.filtered)


# create a table of the final model outputs, and save as a word .doc
tab_model(model.one, 
          show.df = TRUE,
          show.se = TRUE, 
          string.se = "SE",
          dv.labels = c("Model 1"),
          file = "Pitch Ball Velocity Models.doc")
```

```{r}
# summarise the strongest model
summary(model.one)

model_performance(model.one)
```

## Conclusion 

The linear combination of __Lead.Ankle.Flexion.at.FC..deg.__, __Max.Lead.Hip.Flexion.Velocity..deg.s.:Lead.Ankle.Flexion.at.FC..deg.__, and __Max.Lead.Hip.Flexion.Velocity..deg.s.:Step.Width..m.__ accounted for ~26% of the variance in pitched ball velocity. For every 1 unit change in each term (respectively), pitched ball velocity is only expected to change minimally (< 0.1 m/s). Considering that there is still 74% of variance unexplained, we might conclude that biomechanical variables other than those identified here may do a better job of explaining pitched ball velocity in baseball.